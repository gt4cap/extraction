#!/usr/bin/env python
# coding: utf-8

# #Â Query And Transfer CARD Metadata with XMLHttpRequests (CREODIAS)
# 
# CREODIAS ingests the metadata for the generated CARD data in the [CREODIAS public catalog](http://finder.creodias.eu)
# 
# The catalog interface uses the OpenSearch API, a standard to define catalog queries with a set of standards query parameters, and which produces either JSON or XML formatted response which we can parse into our data base.
# 
# In this notebook, we show how to find the data and transfer the relevant metadata records and attributes to the DIAS data base. This is primarily done to make sure we can run the same scripts across different DIAS instances using the same data base table (so that portability is easy).
# 
# We apply the concepts from the __SimpleDatabaseQueries__ notebook.
#
# Revision: 2019-09-03: Switch from georss to gml geometry parsing (georss does not support MULTIPOLYGON geometry, introduced by ESA mongols in 2019.
#
import sys
import requests
from lxml import etree
import psycopg2
from datetime import datetime

# Define the query string for the DIAS catalog search

# Get the bounding polygon for the Area of Interest
# This is Catalunya (ES) = """POLYGON((0.58+40.5,3.33+41.8,3.33+42.5,0.61+42.9,0.04+40.7,0.58+40.5))"""
# denmark = POLYGON((8.0+54.8,8.0+57.0,10.7+57.8,13.2+55.2,15.3+55.3,15.4+54.9,12.0+54.5,8.0+54.7,8.0+54.8))
# NL + NRW
aoi = """POLYGON((3.0+51.27,6.49+50.27,9.66+51.22,9.135+52.618,7.28+52.47,7.213+53.515,4.774+53.56,3.0+51.27))"""       
# NOUR ES
#aoi = """POLYGON((0.8+41.6,1.2+41.6,1.2+41.9,0.8+41.9,0.8+41.6))"""
# Query must be one continuous line, without line breaks!!
url = """https://finder.creodias.eu/resto/api/collections/Sentinel1/search.atom?maxRecords=2000&startDate={}&completionDate={}&productType={}&sortParam=startDate&sortOrder=descending&status=all&geometry={}&dataset=ESA-DATASET"""

# We adapt the period to make sure we get no more than 2000 records (the maximum number of return records).
# For Catalonya, we expect in the order of 600 records for CARD-BS
startDate = '{}T00:00:00Z'.format(sys.argv[1])
endDate = '{}T00:00:00Z'.format(sys.argv[2])
ptype = sys.argv[3]   # CARD-COH6 or CARD-BS
card = sys.argv[4]    # c6 or bs

url = url.format(startDate, endDate, ptype, aoi)

r = requests.get(url)

contentType = r.headers.get('content-type').lower()

# If all goes well, we should get 'application/atom.xml' as contentType
print(contentType)


# Prepare the database for record ingestion

insertSql = """
    INSERT into dias_catalogue (obstime, reference, sensor, card, footprint) 
    values ('{}'::timestamp, '{}', '{}', '{}', ST_GeomFromText('{}', 4326))
    """

if contentType.find('xml')==-1:
    print("FAIL: Server does not return XML content for metadata, but {}.".format(contentType))
    print(r.content)
else:
    # Some special response handling
    respString = r.content.decode('utf-8').replace('resto:','').replace('sentinel:', '')
    
    meta = etree.fromstring(bytes(respString, encoding = 'utf-8')) # replace required to avoid namespace bug
    
    entries = meta.xpath('//a:entry', namespaces = {'a' : 'http://www.w3.org/2005/Atom'})
    print(len(entries))
    

if len(entries) > 0:
    try:
        conn = psycopg2.connect("dbname='postgres' user='postgres' host='172.17.0.2' password='DEMO4eosc'")
    except:
        print("I am unable to connect to the database")
        sys.exit(1)

cur = conn.cursor()

# The XML parsing will select relevant metadata parameters and reformats these into records to insert into the __dias_catalogue__ table. 
# Note that rerunning the parsing will skip records that are already in the table with an existing reference attribute (the primary key).
# Rerunning will, thus, only add new records.
# 

for e in entries:
    title = e.xpath('a:title', namespaces = {'a' : 'http://www.w3.org/2005/Atom'})
    #print(title[0].text)
    sensor = title[0].text[1:3].strip()
    #print(sensor)
    tstamp = e.xpath('gml:validTime/gml:TimePeriod/gml:beginPosition', namespaces = {'gml' : 'http://www.opengis.net/gml'})
    #print(tstamp[0].text)
    polygon = e.xpath('.//gml:coordinates', namespaces = {'gml' : 'http://www.opengis.net/gml'})
    try:
        coords = polygon[0].text
        #print(coords)
        footprint = 'POLYGON(({}))'.format(coords.replace(' ',';').replace(',',' ').replace(';',','))
        try:
            cur.execute(insertSql.format(tstamp[0].text.replace('T', ' '), title[0].text, sensor, card, footprint))
            conn.commit()
        except psycopg2.IntegrityError:
            print("Skipping duplicate record!")
            conn.rollback()

    except:
        print("Polygon parsing issues for {} with polygon {}".format(title[0].text, polygon))

# Important attributes in the __dias_catalogue__ table are:
# 
#  - _reference_: this is the unique reference, with which the S3 object storage key to locate the relevant file(s) can be created;
#  - _obstime_: the image acquisition time stamp (UTC);
#  - _sensor_: the sensor (1A, 1B, 2A or 2B);
#  - _card_: this is the CARD type. Together with _sat_ they point to the expected CARD types, but _type_ is unique already;
#  - _footprint_: this is the footprint geometry of the CARD image;
#  
# Note that, for Sentinel-1, we do not store the orbit direction (_ASCENDING_ or _DESCENDING_). As a general rule, UTC time stamps in the (local) morning are for descending orbits, in the evening for ascending orbits.
# 

# Get some statistics on CARD types that are available for this area of interest
getMetadataSql = """
    select card, sensor, count(*), min(obstime), max(obstime) from dias_catalogue 
    where st_intersects(footprint, st_geomfromtext('{}', 4326))
    group by card, sensor order by card, sensor;
""".format(aoi.replace('+',' '))

cur.execute(getMetadataSql)
# Get the columns names for the rows
colnames = [desc[0] for desc in cur.description]
print(colnames)

for rows in cur:
    print(rows[0:3], datetime.strftime(rows[3], '%Y-%m-%d %H:%M:%S'), datetime.strftime(rows[4], '%Y-%m-%d %H:%M:%S'))   
    

# Each record get the _status_ 'ingested' by default. 
# Close database connection

cur.close()
conn.close()
